{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNIvSqgJYU5Q",
        "outputId": "e288c029-49a0-4ac8-823a-16f2a9e0af92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting test.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile test.cu\n",
        "#include <iostream>\n",
        "#include <cuda_runtime.h>\n",
        "__global__ void addVectors(int* A, int* B, int* C, int n) {\n",
        "int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "if (i < n) {\n",
        "C[i] = A[i] + B[i];\n",
        "}\n",
        "}\n",
        "int main() {\n",
        "int n;\n",
        "std::cout << \"Enter the number of elements: \";\n",
        "std::cin >> n;\n",
        "int* A = new int[n];\n",
        "int* B = new int[n];\n",
        "int* C = new int[n];\n",
        "std::cout << \"Enter elements for vector A:\" << std::endl;\n",
        "for (int i = 0; i < n; i++) {\n",
        "std::cin >> A[i];\n",
        "}\n",
        "std::cout << \"Enter elements for vector B:\" << std::endl;\n",
        "for (int i = 0; i < n; i++) {\n",
        "std::cin >> B[i];\n",
        "}\n",
        "int size = n * sizeof(int);\n",
        "int* dev_A, * dev_B, * dev_C;\n",
        "cudaMalloc(&dev_A, size);\n",
        "cudaMalloc(&dev_B, size);\n",
        "cudaMalloc(&dev_C, size);\n",
        "cudaMemcpy(dev_A, A, size, cudaMemcpyHostToDevice);\n",
        "cudaMemcpy(dev_B, B, size, cudaMemcpyHostToDevice);\n",
        "int blockSize = 256;\n",
        "int numBlocks = (n + blockSize - 1) / blockSize;\n",
        " addVectors<<<numBlocks, blockSize>>>(dev_A, dev_B, dev_C, n);\n",
        "    cudaError_t err = cudaGetLastError();\n",
        "    if (err != cudaSuccess) {\n",
        "        std::cerr << \"Kernel launch failed: \" << cudaGetErrorString(err) << std::endl;\n",
        "        return 1;\n",
        "    }\n",
        "\n",
        "    cudaDeviceSynchronize(); // Ensure kernel completes\n",
        "cudaMemcpy(C, dev_C, size, cudaMemcpyDeviceToHost);\n",
        "std::cout << \"Vector Addition Results:\" << std::endl;\n",
        "for (int i = 0; i < n && i < 10; i++) { //print up to 10 results.\n",
        "std::cout << C[i] << \" \";\n",
        "}\n",
        "std::cout << std::endl;\n",
        "cudaFree(dev_A);\n",
        "cudaFree(dev_B);\n",
        "cudaFree(dev_C);\n",
        "delete[] A;\n",
        "delete[] B;\n",
        "delete[] C;\n",
        "return 0;\n",
        "}\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc test.cu -o test\n",
        "\n",
        "\n",
        "!nvcc -arch=sm_75 -o test test.cu\n",
        "\n",
        "\n",
        "!./test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MG0OK4GPcwAr",
        "outputId": "b96cb248-c16a-41a9-98b0-8b2f78a4785c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the number of elements: 5\n",
            "Enter elements for vector A:\n",
            "1 2 3 4 5\n",
            "Enter elements for vector B:\n",
            "10 20 30 40 50\n",
            "Vector Addition Results:\n",
            "11 22 33 44 55 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "This CUDA C++ program performs vector addition in parallel using a GPU. It takes two arrays (A and B) of integers from the user, adds them element-wise on the GPU, and prints the result.\n",
        "\n",
        "ðŸ§  Section-wise Explanation\n",
        "1. Includes & Setup\n",
        "cpp\n",
        "Copy\n",
        "Edit\n",
        "#include <iostream>\n",
        "#include <cuda_runtime.h>\n",
        "iostream: Standard C++ header for input and output.\n",
        "\n",
        "cuda_runtime.h: Includes CUDA-specific functions like cudaMalloc, cudaMemcpy, etc.\n",
        "\n",
        "2. The CUDA Kernel Function\n",
        "cpp\n",
        "Copy\n",
        "Edit\n",
        "__global__ void addVectors(int* A, int* B, int* C, int n)\n",
        "__global__: CUDA keyword marking this function as a kernel â€” callable from host (CPU) and executed on the device (GPU).\n",
        "\n",
        "int i = blockIdx.x * blockDim.x + threadIdx.x;: This calculates a unique thread index based on:\n",
        "\n",
        "blockIdx.x: Block number in the grid\n",
        "\n",
        "blockDim.x: Threads per block\n",
        "\n",
        "threadIdx.x: Thread number in the block\n",
        "\n",
        "ðŸ’¡ Concept: Thread Hierarchy\n",
        "\n",
        "CUDA uses a hierarchy: Grid â†’ Blocks â†’ Threads. Each thread executes the kernel independently.\n",
        "\n",
        "3. User Input & Memory Allocation\n",
        "cpp\n",
        "Copy\n",
        "Edit\n",
        "int* A = new int[n];\n",
        "Uses dynamic memory allocation (heap) to allow runtime-determined size.\n",
        "\n",
        "Prompts the user to input elements for vectors A and B.\n",
        "\n",
        "ðŸ§  Concept: Host vs Device Memory\n",
        "\n",
        "Host memory: regular CPU RAM (e.g., A, B, C).\n",
        "\n",
        "Device memory: GPU RAM (e.g., dev_A, dev_B, dev_C).\n",
        "\n",
        "4. Memory Copy (Host to Device)\n",
        "cpp\n",
        "Copy\n",
        "Edit\n",
        "cudaMemcpy(dev_A, A, size, cudaMemcpyHostToDevice);\n",
        "Transfers data from the CPU to GPU.\n",
        "\n",
        "Required before computation as the GPU cannot directly access host memory.\n",
        "\n",
        "ðŸ’¡ Concept: cudaMemcpy\n",
        "\n",
        "cudaMemcpyHostToDevice: Copy from CPU to GPU.\n",
        "\n",
        "cudaMemcpyDeviceToHost: Copy from GPU to CPU.\n",
        "\n",
        "5. Kernel Launch\n",
        "cpp\n",
        "Copy\n",
        "Edit\n",
        "addVectors<<<numBlocks, blockSize>>>(dev_A, dev_B, dev_C, n);\n",
        "<<<numBlocks, blockSize>>>: Launch configuration. Specifies how many blocks and threads per block to use.\n",
        "\n",
        "ðŸ’¡ Formula:\n",
        "\n",
        "cpp\n",
        "Copy\n",
        "Edit\n",
        "numBlocks = (n + blockSize - 1) / blockSize;\n",
        "Ensures all n elements are processed, even if n isn't divisible by blockSize.\n",
        "\n",
        "6. Error Checking and Synchronization\n",
        "cpp\n",
        "Copy\n",
        "Edit\n",
        "cudaError_t err = cudaGetLastError();\n",
        "Checks if the kernel was launched correctly.\n",
        "\n",
        "cudaDeviceSynchronize() waits until the GPU finishes execution before moving on.\n",
        "\n",
        "ðŸ’¡ Concept: Asynchronous Execution\n",
        "\n",
        "CUDA kernel launches are asynchronous by default; synchronization ensures predictable behavior.\n",
        "\n",
        "7. Result Transfer and Cleanup\n",
        "cpp\n",
        "Copy\n",
        "Edit\n",
        "cudaMemcpy(C, dev_C, size, cudaMemcpyDeviceToHost);\n",
        "Brings result back to host.\n",
        "\n",
        "Only prints first 10 results (for brevity).\n",
        "\n",
        "Frees device and host memory.\n",
        "\n",
        "ðŸ“˜ Related Computer Science Concepts\n",
        "Here are some broader concepts this code relates to:\n",
        "\n",
        "1. Parallel Computing\n",
        "Each GPU thread handles a single element of the vectors.\n",
        "\n",
        "Drastically improves performance for large n.\n",
        "\n",
        "2. Memory Management\n",
        "The code demonstrates manual memory management (both host and device), which is crucial in systems programming and performance optimization.\n",
        "\n",
        "3. Concurrency vs Parallelism\n",
        "Concurrency is about dealing with multiple tasks at once (not necessarily simultaneous).\n",
        "\n",
        "Parallelism is actually doing many tasks simultaneously, like GPU threads here.\n",
        "\n",
        "4. Race Conditions\n",
        "This code avoids race conditions by giving each thread its own index i.\n",
        "\n",
        "In more complex scenarios, care must be taken to prevent multiple threads writing to the same memory.\n",
        "\n",
        "5. Hardware Abstraction\n",
        "CUDA abstracts GPU architecture while still exposing thread-level control.\n",
        "\n",
        "You must understand both software and hardware to optimize performance."
      ],
      "metadata": {
        "id": "meWlUom2Y2u5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}