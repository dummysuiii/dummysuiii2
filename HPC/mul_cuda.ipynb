{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNIvSqgJYU5Q",
        "outputId": "73be8712-5d60-4e62-dd38-c83cd458f93e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result matrix C:\n",
            "30 24 18 \n",
            "84 69 54 \n",
            "138 114 90 \n"
          ]
        }
      ],
      "source": [
        "cuda_code = \"\"\"\n",
        "#include <iostream>\n",
        "using namespace std;\n",
        "\n",
        "__global__ void matrixMul(int *A, int *B, int *C, int n) {\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (row < n && col < n) {\n",
        "        int value = 0;\n",
        "        for (int k = 0; k < n; k++) {\n",
        "            value += A[row * n + k] * B[k * n + col];\n",
        "        }\n",
        "        C[row * n + col] = value;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int n = 3;\n",
        "    int A[] = {1, 2, 3, 4, 5, 6, 7, 8, 9};\n",
        "    int B[] = {9, 8, 7, 6, 5, 4, 3, 2, 1};\n",
        "    int C[n * n];\n",
        "\n",
        "    int *d_A, *d_B, *d_C;\n",
        "    cudaMalloc(&d_A, n * n * sizeof(int));\n",
        "    cudaMalloc(&d_B, n * n * sizeof(int));\n",
        "    cudaMalloc(&d_C, n * n * sizeof(int));\n",
        "\n",
        "    cudaMemcpy(d_A, A, n * n * sizeof(int), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_B, B, n * n * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    dim3 blockSize(16, 16);\n",
        "    dim3 gridSize((n + blockSize.x - 1) / blockSize.x, (n + blockSize.y - 1) / blockSize.y);\n",
        "    matrixMul<<<gridSize, blockSize>>>(d_A, d_B, d_C, n);\n",
        "\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    cudaMemcpy(C, d_C, n * n * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    cout << \"Result matrix C:\\\\n\";\n",
        "    for (int i = 0; i < n; i++) {\n",
        "        for (int j = 0; j < n; j++) {\n",
        "            cout << C[i * n + j] << \" \";\n",
        "        }\n",
        "        cout << endl;\n",
        "    }\n",
        "\n",
        "    cudaFree(d_A); cudaFree(d_B); cudaFree(d_C);\n",
        "    return 0;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Save CUDA code to a file\n",
        "with open(\"matrix_mul.cu\", \"w\") as f:\n",
        "    f.write(cuda_code)\n",
        "\n",
        "# Compile the CUDA code for the appropriate architecture (sm_70 for Tesla T4)\n",
        "!nvcc -arch=sm_70 matrix_mul.cu -o matrix_mul\n",
        "\n",
        "# Run the compiled program\n",
        "!./matrix_mul"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "The program performs matrix multiplication (C = A Ã— B) on two square matrices using the GPU via CUDA for parallel computation.\n",
        "\n",
        "ðŸš€ CUDA Kernel\n",
        "cpp\n",
        "Copy\n",
        "Edit\n",
        "__global__ void matrixMul(int *A, int *B, int *C, int n)\n",
        "This is a CUDA kernel, a function that runs on the GPU.\n",
        "\n",
        "The __global__ qualifier tells the compiler that this function can be called from the host (CPU) and executed on the device (GPU).\n",
        "\n",
        "ðŸ§® Matrix Multiplication Logic\n",
        "Matrix multiplication is:\n",
        "\n",
        "bash\n",
        "Copy\n",
        "Edit\n",
        "C[i][j] = sum over k of A[i][k] * B[k][j]\n",
        "Since matrices are stored in row-major order in C/C++, the element access is flattened:\n",
        "\n",
        "cpp\n",
        "Copy\n",
        "Edit\n",
        "A[row * n + k] * B[k * n + col]\n",
        "Each GPU thread computes one value of the result matrix C[row][col].\n",
        "\n",
        "ðŸ’¡ Important Keywords and Concepts Explained\n",
        "Keyword\tExplanation\n",
        "__global__\tCUDA specifier: kernel function run on GPU, called from host\n",
        "blockIdx, threadIdx\tBuilt-in CUDA variables: index of block/thread in grid\n",
        "dim3\tA CUDA struct for 3D block/grid dimensions (x, y, z)\n",
        "cudaMalloc()\tAllocates memory on GPU device\n",
        "cudaMemcpy()\tTransfers memory between host and device\n",
        "cudaDeviceSynchronize()\tWaits until all GPU threads are finished\n",
        "cudaFree()\tFrees memory allocated on GPU\n",
        "nvcc\tNVIDIA CUDA compiler\n",
        "\n",
        "ðŸ§  Broader Computer Science Concepts\n",
        "Here are foundational CS concepts related to this code:\n",
        "\n",
        "1. Parallel Computing\n",
        "CUDA is all about data parallelism: the same operation (matrix multiply) is applied independently to different data (each matrix element).\n",
        "\n",
        "Threads and blocks are the building blocks of parallel work on the GPU.\n",
        "\n",
        "2. Memory Hierarchy\n",
        "CUDA has multiple types of memory:\n",
        "\n",
        "Global memory (cudaMalloc): accessible by all threads, but slower.\n",
        "\n",
        "Shared memory: faster, used within thread blocks (not used in this code but crucial in optimization).\n",
        "\n",
        "Registers/local memory: per-thread private memory.\n",
        "\n",
        "Understanding memory hierarchy is key to optimizing performance.\n",
        "\n",
        "3. Thread Mapping and Indexing\n",
        "Each thread calculates its own output index:\n",
        "\n",
        "cpp\n",
        "Copy\n",
        "Edit\n",
        "int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "This ensures that the grid of threads maps to the 2D output matrix.\n",
        "\n",
        "4. Arithmetic Intensity and Computational Efficiency\n",
        "Matrix multiplication has high arithmetic intensity (many computations per memory access), making it ideal for GPU acceleration.\n",
        "\n",
        "GPUs outperform CPUs in such scenarios due to massive parallelism.\n",
        "\n",
        "5. Synchronization and Race Conditions\n",
        "cudaDeviceSynchronize() ensures all threads complete before copying results back.\n",
        "\n",
        "No race condition in this example because each thread writes to its unique element in C.\n",
        "\n",
        "ðŸ§¾ Sample Output\n",
        "rust\n",
        "Copy\n",
        "Edit\n",
        "Result matrix C:\n",
        "30 24 18\n",
        "84 69 54\n",
        "138 114 90\n",
        "This confirms:\n",
        "\n",
        "plaintext\n",
        "Copy\n",
        "Edit\n",
        "A =  [1 2 3; 4 5 6; 7 8 9]\n",
        "B =  [9 8 7; 6 5 4; 3 2 1]\n",
        "C = A x B"
      ],
      "metadata": {
        "id": "meWlUom2Y2u5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}